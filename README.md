# Hadoop-MapReducer
#### This repository contains MapReduce programs developed using Hadoop, an open-source framework for distributed storage and processing of large datasets. MapReduce is a programming model for processing and generating large datasets that runs on Hadoop. It consists of two main tasks - the Map task and the Reduce task - which are executed across multiple nodes in a Hadoop cluster. 

## Programs
### Wordcount
The WordCount program is a classic example of MapReduce. It counts the occurrences of each word in a given input text file. The MapReduce programs for WordCount in this repository are implemented using both Python and Java. 

#### Java Implementation 
The Java implementation of WordCount consists of mapper and reducer classes written in Java. These classes are compiled into a JAR file for execution on the Hadoop cluster.

#### Python Implementation
The Python implementation of WordCount comprises mapper and reducer functions written in Python. These functions are executed using Hadoop Streaming, which allows MapReduce jobs to be written in any programming language that can read from standard input and write to standard output.

### Files
`mapper.py`: This file contains the mapper function, which processes each line of input and emits key-value pairs.

`reducer.py`: This file contains the reducer function, which aggregates the key-value pairs generated by the mapper and produces the final output.

### Dataset 
The dataset used for the WordCount program can be downloaded from https://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/0/2/2.txt
